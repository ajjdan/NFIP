#-------------------------------------------------------------------------------
# Pre-processing flood Zone data
# Optimized for Large dataset that exceeds RAM capacity
# Flood zone information were downloaded per state as geodatabase
#
# 1- Read the length of the attribute table
# 2- Split the number of objects in 3 and read them seperately
#
# 3- repair geometries of the polygons
# 4- dissolve flood zones
# 5- write shapefile
#
#-------------------------------------------------------------------------------
library(sf)
library(gdalUtils)
library(tidyverse)
library(tigris)
library(data.table)
#-------------------------------------------------------------------------------
# Find folder and geodatabase names
#-------------------------------------------------------------------------------
path_to_files <- "C:/Users/veigel/Documents/Data/FEMA/floodplains/"
# Load states from tigris shapefiles for double-checking
states <- states(cb = TRUE)
state_names <- states$NAME
floodmap_folders <- list.dirs(path_to_files, full.names = FALSE)
floodmap_gdb_names <- floodmap_folders[c(which(grepl("*.gdb",floodmap_folders)))] # select geodatabeses
floodmap_folders <- floodmap_folders[-c(which(grepl("*.gdb",floodmap_folders)))] #remove geodatabases from folder list
floodmap_folders <- na.omit(floodmap_folders[-c(which(!(floodmap_folders %in% state_names)))]) #remove wrong names in Folders
i <- 12
floodmap_gdb_names[i]
i <- 14
floodmap_gdb_names[i]
## load only objectid for subsetting
prepare_iterations <- st_drop_geometry(sf::st_read(
dsn = paste0(path_to_files, floodmap_gdb_names[i]),
layer = "S_FLD_HAZ_AR",
query = "SELECT OBJECTID FROM S_FLD_HAZ_AR"))
rd_st <- Sys.time()
## read and manipulate half of the polygons
floodmap <- sf::st_read(
dsn = paste0(path_to_files, floodmap_gdb_names[i]),
layer = "S_FLD_HAZ_AR",
query = paste0("SELECT FLD_ZONE FROM S_FLD_HAZ_AR WHERE OBJECTID < ", ceiling(nrow(prepare_iterations)/3))
)
floodmap <-
floodmap %>%
st_make_valid() %>%
st_cast("POLYGON") %>%
st_buffer(0) %>%
group_by(FLD_ZONE ) %>%
summarise()
## export shapefile
st_write(floodmap,paste0(path_to_files,"preprocessed/", floodmap_folders[i]), driver = "ESRI Shapefile", append = FALSE )
print(paste0("Time after 1/3 of polygons ", Sys.time()-rd_st, " Estimated time for ", floodmap_folders[i], (Sys.time()-rd_st)*3 ))
## read and manipulate second half of the polygons
floodmap <- sf::st_read(
dsn = paste0(path_to_files, floodmap_gdb_names[i]),
layer = "S_FLD_HAZ_AR",
query =  paste0("SELECT FLD_ZONE FROM S_FLD_HAZ_AR WHERE OBJECTID >= ", ceiling(nrow(prepare_iterations)/3), " AND OBJECTID < ", 2*ceiling(nrow(prepare_iterations)/3))
)
floodmap <-
floodmap %>%
st_make_valid() %>%
st_cast("POLYGON") %>%
st_buffer(0) %>%
group_by(FLD_ZONE ) %>%
summarise()
## export shapefile, appending the polygons
st_write(floodmap,paste0(path_to_files,"preprocessed/", floodmap_folders[i]) ,paste0(floodmap_folders[i],"_2"), driver = "ESRI Shapefile", append = FALSE)
print(paste0("Time after 2/3 of polygons ", Sys.time()-rd_st, " Estimated time for ", floodmap_folders[i], ((Sys.time()-rd_st)/2)*3 ))
floodmap <- sf::st_read(
dsn = paste0(path_to_files, floodmap_gdb_names[i]),
layer = "S_FLD_HAZ_AR",
query =  paste0("SELECT FLD_ZONE FROM S_FLD_HAZ_AR WHERE OBJECTID >= ", 2*ceiling(nrow(prepare_iterations)/3))
)
floodmap <-
floodmap %>%
st_make_valid() %>%
st_cast("POLYGON") %>%
st_buffer(0) %>%
group_by(FLD_ZONE ) %>%
summarise()
## export shapefile, appending the polygons
st_write(floodmap,paste0(path_to_files,"preprocessed/", floodmap_folders[i]),paste0(floodmap_folders[i],"_3"), driver = "ESRI Shapefile" , append = FALSE)
rm(list="floodmap")
gc()
et <- Sys.time()
print(paste0("Time after all ", floodmap_folders[i],"  polygons: ", Sys.time()-et-rd_st))
install.packages("noaastormevents")
